在这个作业中，我们仍然专注于MNIST数字分类问题。在执行MLP和美国有线电视新闻网的详细信息之后，你可能会更多地了解它们。事实上，类似的神经网络与标准结构往往封装为模块在深度学习框架。掌握使用这些模块的技能，并用深度学习框架构建模型，对我们来说既方便又重要。

你将被允许使用像TensorFlow这样的框架。我们希望您能够理解它们的特性（例如TensorFlow中的数据流图），并实现MLP和CNN来完成MNIST数字分类的任务。


此外，还应实现另外2种技术。

辍学，其目的是防止过度拟合。在训练过程中，单个节点要么以概率“退出”网络，要么以概率保持，从而留下一个减少的网络，并且到退出节点的传入和传出边缘也被去除。在测试时，我们理想地希望找到所有可能的丢弃网络的样本平均值；不幸的是，对于。然而，我们可以使用每个节点的输出加权一个因子的全网络来找到近似值，因此任何节点的输出的期望值都与训练阶段相同。
在这个代码中，我们以另一种方式实现辍学。在训练过程中，我们按比例扩展剩余网络节点的输出。在测试时间，我们在辍学层中什么也不做。很容易发现这种方法对原始辍学有类似的结果。

批量归一化，其目的是处理内部协变量换档问题。具体而言，在训练过程中，每层输入的分布将随着前一层参数的变化而变化。研究者建议对每个神经元的激活函数进行批量归一化，使得每个小批量的输入均值为0，方差为1。在一个小批量中正常化一个值，



CNN的批量归一化必须服从卷积性质，因此同一特征映射中的不同单元必须以相同的方式进行归一化。参见参考文献〔2〕。

当我们逐一测试我们的样本时，标准化可能不起作用，因为我们这次没有小批量。在训练过程中应保持总体均值和方差，并在测试时使用“均值”和“方差”进行估计。简单地说，你可以尝试计算移动平均值，当训练时，把它们作为总体均值和方差。参见参考文献〔2〕。




在实验报告中，你需要回答以下基本问题：
1。写下如何填充模型的参数。解释如何训练和重用工作。为什么培训和测试会有所不同？
2。将损失值（训练损失和验证损失）与训练过程中的每一次迭代相结合。
三。构建具有批归一化和辍学的多层感知器和卷积神经网络。比较MLP和美国有线电视新闻网的结果之间的差异。
4。建立无批次标准化的MLP和美国有线电视新闻网，并讨论批量归一化的影响。
5。调整辍学率，并讨论辍学的影响。
6。解释为什么培训损失和验证损失是不同的。差异如何帮助你调整超参数？